{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test with multiple layers and multi-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/jaja/miniconda3/envs/deepenv/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from mltools import *\n",
    "from activation import TanH, Sigmoid, Softmax, LogSoftmax\n",
    "from Linear import Linear\n",
    "from Loss import *\n",
    "from module import Module\n",
    "from encapsulate import Optim, Sequential\n",
    "np.random.seed(42)\n",
    "plt.rcParams['figure.dpi'] = 500\n",
    "plt.rcParams['savefig.dpi'] = 500"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of 10 classes: how many layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "X, y = load_digits(return_X_y=True, n_class=9)\n",
    "y_oh = OneHotEncoder().fit_transform(y.reshape(-1, 1)).toarray()\n",
    "\n",
    "\n",
    "\n",
    "# one hidden layer\n",
    "# net = Sequential(\n",
    "#                 Linear(64, 32),\n",
    "#                 TanH(),\n",
    "#                 Linear(32, 10),\n",
    "#                 Sigmoid(),\n",
    "#             )\n",
    "\n",
    "# print(net.modules)\n",
    "\n",
    "# net.insert(2, Linear(32, 32))\n",
    "# net.insert(1, TanH())\n",
    "# print(net.modules)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "dimension mismatch, y and yhat must of same dimension. Here it is (21, 9) and (21, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4386/1213255110.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCELogSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m result_df = optimizer.SGD_eval(\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_oh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dataframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss_train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"hidden layers = 1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/encapsulate.py\u001b[0m in \u001b[0;36mSGD_eval\u001b[0;34m(self, X, y, batch_size, epochs, test_size, patience, network, shuffle_train, shuffle_test, seed, return_dataframe, online_plot)\u001b[0m\n\u001b[1;32m    166\u001b[0m             )\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mX_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                 \u001b[0mloss_batch_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m                 \u001b[0mloss_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_batch_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/encapsulate.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, batch_x, batch_y)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mloss_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, y, yhat)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         assert y.shape == yhat.shape, ValueError(\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;34mf\"dimension mismatch, y and yhat must of same dimension. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;34mf\"Here it is {y.shape} and {yhat.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         )\n",
      "\u001b[0;31mAssertionError\u001b[0m: dimension mismatch, y and yhat must of same dimension. Here it is (21, 9) and (21, 10)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 5000x5000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmap = plt.get_cmap('viridis')\n",
    "colors = np.linspace(0, 1, 6)\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "num_epochs = 1000\n",
    "eps = 1e-2\n",
    "\n",
    "net = Sequential(\n",
    "                Linear(64, 32),\n",
    "                TanH(),\n",
    "\n",
    "                Linear(32, 10),\n",
    "\n",
    "                LogSoftmax(),\n",
    "            )\n",
    "\n",
    "print(len(net.modules))\n",
    "\n",
    "\n",
    "optimizer = Optim(net.reset(), CELogSoftmax(), eps=eps)\n",
    "result_df = optimizer.SGD_eval(\n",
    "        X, y_oh, batch_size, 1000, test_size=10, return_dataframe=True\n",
    "    )\n",
    "plt.plot(np.arange(0,1000,1), result_df[\"loss_train\"], label = f\"hidden layers = 1\", color = cmap(colors[0]))\n",
    "plt.plot(np.arange(0,1000,1), result_df[\"loss_test\"], color = cmap(colors[0]), alpha=0.4)\n",
    "\n",
    "net = Sequential(\n",
    "                Linear(64, 32),\n",
    "                TanH(),\n",
    "                Linear(32, 32),\n",
    "                TanH(),\n",
    "\n",
    "\n",
    "                Linear(32, 10),\n",
    "\n",
    "                LogSoftmax(),\n",
    "            )\n",
    "\n",
    "print(len(net.modules))\n",
    "\n",
    "\n",
    "optimizer = Optim(net.reset(), CELogSoftmax(), eps=eps)\n",
    "result_df = optimizer.SGD_eval(\n",
    "        X, y_oh, batch_size, 1000, test_size=10, return_dataframe=True\n",
    "    )\n",
    "plt.plot(np.arange(0,1000,1), result_df[\"loss_train\"], label = f\"hidden layers = 2\", color = cmap(colors[1]))\n",
    "plt.plot(np.arange(0,1000,1), result_df[\"loss_test\"], color = cmap(colors[1]), alpha=0.4)\n",
    "\n",
    "net = Sequential(\n",
    "                Linear(64, 32),\n",
    "                TanH(),\n",
    "                Linear(32, 32),\n",
    "                TanH(),\n",
    "                Linear(32, 32),\n",
    "                TanH(),\n",
    "                \n",
    "\n",
    "                Linear(32, 10),\n",
    "\n",
    "                LogSoftmax(),\n",
    "            )\n",
    "\n",
    "print(len(net.modules))\n",
    "\n",
    "\n",
    "optimizer = Optim(net.reset(), CELogSoftmax(), eps=eps)\n",
    "result_df = optimizer.SGD_eval(\n",
    "        X, y_oh, batch_size, 1000, test_size=10, return_dataframe=True\n",
    "    )\n",
    "plt.plot(np.arange(0,1000,1), result_df[\"loss_train\"], label = f\"hidden layers = 3\", color = cmap(colors[2]))\n",
    "plt.plot(np.arange(0,1000,1), result_df[\"loss_test\"], color = cmap(colors[2]), alpha=0.4)\n",
    "\n",
    "net = Sequential(\n",
    "                Linear(64, 32),\n",
    "                TanH(),\n",
    "                Linear(32, 32),\n",
    "                TanH(),\n",
    "                Linear(32, 32),\n",
    "                TanH(),\n",
    "                Linear(32, 32),\n",
    "                TanH(),\n",
    "                \n",
    "\n",
    "                Linear(32, 10),\n",
    "\n",
    "                LogSoftmax(),\n",
    "            )\n",
    "\n",
    "print(len(net.modules))\n",
    "\n",
    "\n",
    "optimizer = Optim(net.reset(), CELogSoftmax(), eps=eps)\n",
    "result_df = optimizer.SGD_eval(\n",
    "        X, y_oh, batch_size, 1000, test_size=10, return_dataframe=True\n",
    "    )\n",
    "plt.plot(np.arange(0,1000,1), result_df[\"loss_train\"], label = f\"hidden layers = 4\", color = cmap(colors[3]))\n",
    "plt.plot(np.arange(0,1000,1), result_df[\"loss_test\"], color = cmap(colors[3]), alpha=0.4)\n",
    "\n",
    "net = Sequential(\n",
    "                Linear(64, 32),\n",
    "                TanH(),\n",
    "                Linear(32, 32),\n",
    "                TanH(),\n",
    "                Linear(32, 32),\n",
    "                TanH(),\n",
    "                Linear(32, 32),\n",
    "                TanH(),\n",
    "                Linear(32, 32),\n",
    "                TanH(),\n",
    "                \n",
    "\n",
    "                Linear(32, 10),\n",
    "\n",
    "                LogSoftmax(),\n",
    "            )\n",
    "\n",
    "print(len(net.modules))\n",
    "\n",
    "\n",
    "optimizer = Optim(net.reset(), CELogSoftmax(), eps=eps)\n",
    "result_df = optimizer.SGD_eval(\n",
    "        X, y_oh, batch_size, 1000, test_size=10, return_dataframe=True\n",
    "    )\n",
    "plt.plot(np.arange(0,1000,1), result_df[\"loss_train\"], label = f\"hidden layers = 5\", color = cmap(colors[4]))\n",
    "plt.plot(np.arange(0,1000,1), result_df[\"loss_test\"], color = cmap(colors[4]), alpha=0.4)\n",
    "\n",
    "net = Sequential(\n",
    "                Linear(64, 32),\n",
    "                TanH(),\n",
    "                Linear(32, 32),\n",
    "                TanH(),\n",
    "                Linear(32, 32),\n",
    "                TanH(),\n",
    "                Linear(32, 32),\n",
    "                TanH(),\n",
    "                Linear(32, 32),\n",
    "                TanH(),\n",
    "                Linear(32, 32),\n",
    "                TanH(),\n",
    "                \n",
    "\n",
    "                Linear(32, 10),\n",
    "\n",
    "                LogSoftmax(),\n",
    "            )\n",
    "\n",
    "print(len(net.modules))\n",
    "\n",
    "\n",
    "optimizer = Optim(net.reset(), CELogSoftmax(), eps=eps)\n",
    "result_df = optimizer.SGD_eval(\n",
    "        X, y_oh, batch_size, 1000, test_size=10, return_dataframe=True\n",
    "    )\n",
    "plt.plot(np.arange(0,1000,1), result_df[\"loss_train\"], label = f\"hidden layers = 6\", color = cmap(colors[5]))\n",
    "plt.plot(np.arange(0,1000,1), result_df[\"loss_test\"], color = cmap(colors[5]), alpha=0.4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.grid()\n",
    "plt.title(\"Loss function in train and test \\n for different number of hidden layers \\n 9 classes classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
